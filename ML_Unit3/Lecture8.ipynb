{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ccb20bf-b91d-4e2e-b2bf-7cd700334443",
   "metadata": {},
   "source": [
    "## Lecture 8: Introduction to feedforward neural networks\n",
    "\n",
    "**Motivation**\n",
    "\n",
    "The traditional linear and non-linear classifiers that we have seen so far project the data into a **fixed** feature representation. For example in non-linear classification the classifier used to be of the form:\n",
    "\n",
    "$sign(\\theta.\\phi(x))$, where $\\phi(x)$ was always a fixed feature transformation. In Neural Networks we try to learn both:\n",
    "\n",
    "1. The feature transformation $\\phi(x)$\n",
    "2. The ML task (Classification or Regression)\n",
    "\n",
    "**Neural Network Units, Introduction to deep neural networks**\n",
    "\n",
    "Refer to the slide `./decks/Neural Networks.pptx` The idea of activation functions, flow of data through the network etc is described from slide 1 to 26.\n",
    "\n",
    "The rest of the slides build on the intuition about what parameter learning means for neural networks, builds the intution about the loss functions and data flow. Refer to the `./excel/Numerical Examples.xlsx` for more detailed explanation.\n",
    "\n",
    "**Coding a forward pass**\n",
    "\n",
    "The forward pass of any neural network can be easily represented as a sequence of matrix multiplication steps.\n",
    "\n",
    "![](./imgs/nn.png)\n",
    "\n",
    "This network can be described as follows:\n",
    "\n",
    "- Input vector = $X = (x1,x2)$\n",
    "- Weight Matrix (hidden layer) = $$W = \\begin{bmatrix}\n",
    "W_{11}&&W_{21}\\\\\n",
    "W_{12}&&W_{22}\\\\\n",
    "W_{13}&&W_{23}\\\\\n",
    "W_{14}&&W_{24}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "*note the subscripts are being mapped to weights in the figure\n",
    "\n",
    "- Bias/offset Matrix (hidden layer) = $$\n",
    "W_0 = \\begin{bmatrix}\n",
    "W_{01}\\\\\n",
    "W_{02}\\\\\n",
    "W_{03}\\\\\n",
    "W_{04}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now the forward pass for the hidden layer can be described as \n",
    "\n",
    "$$W \\times X + W_0= Z = \\begin{bmatrix}\n",
    "z_{1}\\\\\n",
    "z_{2}\\\\\n",
    "z_3\\\\\n",
    "z_4\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Applying the activation function $f$ over the matrix $Z$ will complete the forward pass.\n",
    "\n",
    "$$f(W \\times X + W_0)= f(Z) = f(\\begin{bmatrix}\n",
    "z_{1}\\\\\n",
    "z_{2}\\\\\n",
    "z_3\\\\\n",
    "z_4\\\\\n",
    "\\end{bmatrix}) = \n",
    "\\begin{bmatrix}\n",
    "f(z_{1})\\\\\n",
    "f(z_{2})\\\\\n",
    "f(z_3)\\\\\n",
    "f(z_4)\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For the output layer:\n",
    "\n",
    "- The weight matrix is $$V = \\begin{bmatrix}\n",
    "V_{11}&&V_{21}&&V_{31}&&V_{41}\\\\\n",
    "V_{12}&&V_{22}&&V_{32}&&V_{42}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- The bias/offset matrix is $$V_0 = \\begin{bmatrix}\n",
    "V_{01}\\\\\n",
    "V_{02}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The rest of the forward pass can be described as follows:\n",
    "\n",
    "$$softmax(V \\times f(Z) + V_0) = softmax(U) = \\begin{bmatrix} \n",
    "\\frac{e^{(u_1)}}{e^{(u_1)}+e^{(u_2)}}\\\\\n",
    "\\frac{e^{(u_2)}}{e^(u_1)+e^{(u_2)}}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Lets see a piece of code to impliment the above math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "721eee66-4a5f-450d-bfcc-a0b584f0590f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15],\n",
       "       [ 0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[3,14]])\n",
    "W = np.array([[1,0],\n",
    "             [0,1],\n",
    "             [-1,0],\n",
    "             [0,-1]])\n",
    "W0 = np.array([[-1,-1,-1,-1]])\n",
    "V = np.array([[1,1,1,1],\n",
    "             [-1,-1,-1,-1]])\n",
    "V0 = np.array([[0,2]])\n",
    "\n",
    "def relu(z):\n",
    "    z[z<0]=0\n",
    "    return z\n",
    "def softmax(z):\n",
    "    z = np.exp(z)\n",
    "    z = z/z.sum(axis=0)\n",
    "    return z\n",
    "\n",
    "relu(V@(relu(W@X.T+W0.T))+V0.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93503ea2-a2c1-431e-ba25-7847a4668481",
   "metadata": {},
   "source": [
    "One can also write the same logic and encapsulate it in a python class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "27575a4f-4390-4ece-91cc-e8baab40e947",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN():\n",
    "    \n",
    "    def __init__(self,W,W0,V,V0,X):\n",
    "        self.W = W\n",
    "        self.W0 = W0\n",
    "        self.V = V\n",
    "        self.V0 = V0\n",
    "        self.X = X\n",
    "    def relu(self,z):\n",
    "        z[z<0]=0\n",
    "        return z\n",
    "    def forward(self):\n",
    "        return relu(self.V@(self.relu(self.W@self.X.T+self.W0.T))+self.V0.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dba8ebd6-21d4-437c-b9c9-f3f348f9bea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(W,W0,V,V0,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "34a4b47b-92a2-4e72-9fbe-86b14772c039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15],\n",
       "       [ 0]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494b5855-f396-4d72-a0a3-8d243d9da109",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
