{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1010287a-31df-43be-b0e5-532ce9ce9e6d",
   "metadata": {},
   "source": [
    "# Lecture 5 Linear Regression\n",
    "\n",
    "## Introduction\n",
    "\n",
    "- Classification can't handle every problem e.g. predicting continuous values\n",
    "- Regression specification:\n",
    "    - data : $\\{x^i,y^i\\} $, $y \\in R$\n",
    "    - model : $f(x, \\theta_{1}, \\theta_{0}) =  \\sum_{t=1}^{n} \\theta_{1}X_{t}+\\theta_{0}$ \n",
    "    - Non linearity is handled using feature transforms (Kernel methods etc)\n",
    "\n",
    "## Empirical Risk:\n",
    "- $R_{n}(\\theta) = \\frac{1}{n}\\sum_{t=1}^{n}Loss(y^t-\\theta.x^t) $\n",
    "- loss: MSE $\\frac{1}{2n}\\sum_{t=1}^{n}(y^t - (\\theta_{1}X^t + \\theta_{0}))^2$\n",
    "- This loss makes sure that small deviations are not penalized as much as larger deviations from actual and predicted values\n",
    "\n",
    "**Problem Ideas** \n",
    "- Numeric computation of loss given predicted vs actual\n",
    "- Numeric computation of loss given $\\theta$, $X$ and $Y$\n",
    "- Comparison of loss when hinge loss or mse definition is used\n",
    "\n",
    "## Gradient Based Approach:\n",
    "- Gradient of MSE for a single example: $-(y^t-\\theta_{1}.x^t).x^t$\n",
    "\n",
    "**Problem Ideas**\n",
    "- Recall the gradient descent on a simple function such as ```math \n",
    "$f(\\theta) = 2.\\theta - \\theta^2$\n",
    "```\n",
    "- Show one iteration by hand\n",
    "- Create dummy data and run 2 iterations by hand, focus on random selection of a single data point $(x^t,y^t)$\n",
    "- Attempt to give intuition of a random batch update\n",
    "\n",
    "\n",
    "## Closed form solution\n",
    "\n",
    "**Content pointers**\n",
    "- Find all possible variations of closed form derivation\n",
    "- Attempt to give a general rule to create vectorized expressions\n",
    "\n",
    "## Generalization and regularization\n",
    "\n",
    "**Content Pointers**\n",
    "- Use the graph where underlying data comes from a linear function with some noise added and argue that minimizing loss purely for training data will lead to learning random noise as well hence regularization is a good way to counter this.\n",
    "\n",
    "## Regularization\n",
    "\n",
    "**Content Pointers**\n",
    "- Correlate the formulation with the data having trend+random noise, show effect of lambda on the parameter estimate\n",
    "- Correlate the same idea with a sales example over years.\n",
    "- Working example on creating a closed form solution with regularization as well as a gradient based solution.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41995f96-7881-427d-bd9a-6b0f07e2277a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
