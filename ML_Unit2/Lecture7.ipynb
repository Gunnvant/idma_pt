{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c79ee378-467a-49ff-acde-21db86c44670",
   "metadata": {},
   "source": [
    "## Lecture 7: Recommender Systems\n",
    "\n",
    "These notes are for lecture 7, recommendation systems. There are two main ideas that this lecture talks about:\n",
    "\n",
    "1. Using a naive approach based on simple similarity measures to tackle the problem of recommendation\n",
    "2. Using a technique called matrix factorization\n",
    "\n",
    "**Introduction and Notation**\n",
    "\n",
    "The user ratings and the corresponding items are stored in a user-item rating matrix:\n",
    "\n",
    "$$ Y = \n",
    "\\begin{bmatrix}\n",
    "y_{11} & y_{12} & y_{13}\\\\\n",
    "y_{21} & y_{22} & y_{23}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Here $y_{ij}$ corresponds to the rating by $user_i$ for $item_j$\n",
    "\n",
    "There are some peculiarities about this matrix:\n",
    "\n",
    "1. Most entries in this matrix will be missing\n",
    "2. All the users generally don't interact with all the items\n",
    "\n",
    "Check the tab **User Item Rating Matrix** for a detailed example.\n",
    "\n",
    "**K-Nearest Neighbour Method**\n",
    "\n",
    "In this method we do the following:\n",
    "\n",
    "1. We decide on how we will compute the similarity between different users: common metrics that one can use are: euclidean distance, cosine distance, correlation coefficient etc\n",
    "2. We then decide how many similar users should be considered to determine the rating of one target user\n",
    "\n",
    "See the tab **User Item Rating Matrix** for a detailed numeric example\n",
    "\n",
    "The downside of using this approach is that:\n",
    "\n",
    "1. The similarity measures used fail to account for users that can have likings across varied dimensions (ML Academic also liking gardening books)\n",
    "2. The hidden structure of the data is not revealed\n",
    "\n",
    "**Collaborative Filtering: The naive approach**\n",
    "\n",
    "The basic problem set up has been done in the tab called **Collaborative Filtering 1**. Both $Y and X$ matrices have been described there.\n",
    "\n",
    "The estimation problem can be posed in a very similar fashion as the regression problem with following cost function:\n",
    "\n",
    "$Cost(X)=\\sum_{(a,i)\\in D}\\frac{(Y_{ai}-X_{ai})^2}{2}+\\frac{\\lambda}{2}\\sum_{(a,i)}X^2_{ai}$\n",
    "\n",
    "When the cost is minimised, the minima occurs for the following value of $X_{ai}$\n",
    "\n",
    "If $(a,i) \\in D$ then $X_{ai} = \\frac{Y_{ai}}{1+\\lambda}$\n",
    "\n",
    "else\n",
    "\n",
    "$X_{ai}=0$\n",
    "\n",
    "**Collaborative Filtering: Matrix Factorization**\n",
    "\n",
    "The intuition behind the approach is explained in **Algorithms Demos.pptx** deck. The main idea here is that we will factorize the matrix $Y_{m \\times n}$ into two matrices $U_{m \\times k}$ and $V_{k \\times n}$ such that $Y = UV$\n",
    "\n",
    "There are two takeaways from this:\n",
    "\n",
    "1. The rank $k$ lets us decide the number of factors that we may believe will be impacting the rating decisions\n",
    "2. The number of parameters to be estimated are now $m+n$ instead of $m \\times n$.\n",
    "\n",
    "\n",
    "**Alternating Minimization**\n",
    "\n",
    "Refer to the official video, the numerical example is quite clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b7c677-9fe7-4366-9cdf-3a454dcabf52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
