{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e868a827-42c9-460e-b25a-72a77018006c",
   "metadata": {},
   "source": [
    "## Lecture 6: Non Linear Classification Class Notes\n",
    "\n",
    "Data is not always linearly seperable, even if we allow for some slack in terms of accuracy, linear classification methods will not go very far.\n",
    "\n",
    "The good news is we can extend the linear-classification methods very easily to handle non-linear classifcation as well. All we will need to do is, find a way to extend our feature vectors in such a way that they become linearly separable. This lecture is about techniques that help in extending feature vectors to higher dimensions in which the data becomes linearly-separable.\n",
    "\n",
    "**Higher Order Feature Vectors**\n",
    "\n",
    "Feature maps $\\phi(x)$ are the primary theoretical constructs that will help in extending the linear classifiers to handle non-linear problems.\n",
    "\n",
    "Defintion:\n",
    "\n",
    "Let $x$ $\\in R$, then a feature map $\\phi(x)$ will map the data to $R^2$ if it can produce a feature vector given as below provided that it accepts $x \\in R$ as an argument:\n",
    "\n",
    "$\\phi(x) = \\begin{bmatrix}\n",
    "            \\phi_1(x)\\\\\n",
    "            \\phi_2(x)\n",
    "            \\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "x\\\\\n",
    "x^2\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "To understand the intuition as to how a feature map can make a non-linearly separable data, linearly separable refer to the excel sheet tab named **Non Linear Seperability**\n",
    "\n",
    "There are two distinct concepts one needs to understand.\n",
    "\n",
    "1. One can find the new coordinates by using the feature map $\\phi(x)$ on each original point $x$.\n",
    "2. One can also find the shape of the decision boundary in original feature space by tracing the locus of $sign(\\theta.\\phi(x)+\\theta_0)$\n",
    "\n",
    "\n",
    "\n",
    "**Introduction to Non-Linear Classification**\n",
    "\n",
    "Both for classification and regression one can use the idea of feature maps to extend the original features to higher dimensions. The general mechanism by which one can use these higher dimensional features is as follows:\n",
    "\n",
    "- Non linear classification: $sign(\\theta.\\phi(x)+\\theta_0)$\n",
    "- Non linear regression: $\\theta.\\phi(x)+\\theta_0$\n",
    "\n",
    "$\\phi(x)$ can be take many forms for example:\n",
    "\n",
    "1. $\\phi(x) = \\begin{bmatrix}\n",
    "                x\\\\\n",
    "                x^2\\\\\n",
    "                \\end{bmatrix}            \n",
    "               $\n",
    "               \n",
    "               \n",
    "2.  $\\phi(x) = \\begin{bmatrix}\n",
    "                x\\\\\n",
    "                x^2\\\\\n",
    "                x^3\\\\\n",
    "                \\end{bmatrix}            \n",
    "               $\n",
    "               \n",
    "               \n",
    "3.  $\\phi(x) = \\begin{bmatrix}\n",
    "                x\\\\\n",
    "                x^2\\\\\n",
    "                x^3\\\\\n",
    "                x^4\\\\\n",
    "                \\end{bmatrix}            \n",
    "               $\n",
    "               \n",
    "But how can one decide which feature transformation to use? One technique that can be used is k-fold cross validation. The intuition behind this is explained in sheet named **\"cross validation\"**\n",
    "\n",
    "\n",
    "Using feature maps directly might not always be a great idea as there are two major implications of this:\n",
    "\n",
    "1. The number of features explode exponentially if $x \\in R^2$, where $d>=30$.\n",
    "\n",
    "To illustrate this point lets imagine we have a feature map $\\phi(x)$ that maps x to 1st, 2nd and 3rd order polynomial terms if if $x \\in R^2$, where $d>=30$ then the feature vector will have:\n",
    "\n",
    "- Atleast 30 1st order terms\n",
    "- ${30+2-1}\\choose{2}$$=465$, 2nd order terms\n",
    "- ${30+3-1}\\choose{3}$$=4960$, 3rd order terms\n",
    "\n",
    "2. The computation time also increases as the dimensions of the feature vectors increase.\n",
    "\n",
    "**Motivation for Kernel Methods**\n",
    "\n",
    "Since we know that creating explicit feature vectors can be difficult computationally, in this section we will build the ground-work to reduce the computational load.\n",
    "\n",
    "- For some feature maps its very easy to compute the dot product\n",
    "Consider two vectors $x,x' \\in R^2$. Also lets assume a feature transformation $\\phi(x)$ defined as below\n",
    "\n",
    "$\\phi(x) = [x_1,x_2,x_1^2,\\sqrt{2}x_1x_2,x_2^2]$\n",
    "\n",
    "$\\phi(x') = [x_1',x_2',x_1'^2,\\sqrt{2}x_1'x_2',x_2'^2]$\n",
    "\n",
    "Now if we evaluate $\\phi(x).\\phi(x')$, we can see that the following holds:\n",
    "\n",
    "$\\phi(x).\\phi(x') = x.x' + (x.x')^2$\n",
    "\n",
    "The major implications of this example are:\n",
    "\n",
    "1. Dot product of feature vectors can be written as dot product of original vectors\n",
    "2. In case only dot product of feature vectors is needed then there is no need to explicitly create feature vectors and then do the dot product.\n",
    "3. There are certain forms of feature vectors for which above 2 will be true.\n",
    "\n",
    "Another peculiar result is the following:\n",
    "\n",
    "1. If 3 holds then, we generally use a term called kernel function, a kernel function takes original vectors and produces the dot product of specific feature maps applied to the original vectors, i.e.\n",
    "\n",
    "$K(x,x')=\\phi(x).\\phi(x')$\n",
    "\n",
    "\n",
    "**Common Kernel Functions and Decsion Boundaries**\n",
    "\n",
    "1. Polynomial Kernel $(1+x.x')^p$\n",
    "\n",
    "![](./imgs/polynomial.png)\n",
    "\n",
    "2. RBF Kernel $K(x,x') = exp(\\frac{-1}{2}||x-x'||^2)$\n",
    "\n",
    "![](./imgs/rbf.png)\n",
    "\n",
    "\n",
    "**Kernel Perceptron Algorithm**\n",
    "\n",
    "Using the kernel perceptron algorithm one can very easily show that non-linear svm's can easily classify points in higher dimensional space without creating explicit feature vectors but only using kernel functions. See the deck named **Algorithm Demos.pptx**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c8d212-3021-440a-b4f7-20107c49a798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
