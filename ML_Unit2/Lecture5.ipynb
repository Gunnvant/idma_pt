{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d9abc02-9797-4cdb-878b-521d6df672c0",
   "metadata": {},
   "source": [
    "## Lecture 5: Linear Regression Class Notes\n",
    "\n",
    "Not all problems are classification problems. When one wants to predict a continous target variable from a set of features, then the problem is that of regression and not classification. see the tab called **One Feature** in the excel workbook called **Numerical Examples**.\n",
    "\n",
    "**1. Regression Problem**\n",
    "\n",
    "Here the variable we want to predict is continous in nature rather than being a categorical variable \n",
    "\n",
    "The model is $X_t\\theta_1+\\theta_0$\n",
    "\n",
    "**2. Model Meaning**\n",
    "\n",
    "But what does this model mean. Lets explore the tab **Model Meaning**. The model in this case means the parameter values that will enable us to make predictions which are as close to actual values as possible.\n",
    "\n",
    "**3. Parameter Estimation: Cost function**\n",
    "\n",
    "To estimate the value of $\\theta_0$ and $\\theta_1$, we will need to pose this problem as an optimization problem where we should choose those parameter values which minmize the error between actual and predicted values.\n",
    "\n",
    "The error for a single row is defined as $$(\\hat{y_i}-y_i)^2$$ The error for all rows is defined as $$\\frac{1}{2n}\\sum_{0}^{n}(\\hat{y_i}-y_i)^2$$\n",
    "\n",
    "These equations have been summarised in the tab **Cost Function** of the **Numerical Examples** workbook\n",
    "\n",
    "**4. Estimating model parameters: Gradient Descent**\n",
    "\n",
    "Once we are okay with the intuition behind the parameter estimation, we can focus on the specific methods by which the parameters can be estimated. One way to estimate parameters is to use Gradient Descent. The basic update equation for gradient descent is as follows:\n",
    "\n",
    "$\\theta_{new} := \\theta_{old}-\\eta*grad$\n",
    "\n",
    "Refer to the **Gradient Descent Recall** tab in the workbook, for a simple recall of gradient descent.\n",
    "\n",
    "**Class Excecise**\n",
    "Write python code to minimise $\\theta^2-2\\theta$ using gradient descent. Assume that the initial value of $\\theta = -1$, $\\eta=0.01$. Run your iterations 400 times and then print out the values of updated $\\theta$, function's value and value of gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8552e15e-6f30-4c1b-bf98-b5e68c2c2bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    return x**2-2*x\n",
    "def grad(x):\n",
    "    return 2*x-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8da3519a-b03d-4502-874b-4d971b98a85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9999996172452885 -0.0012373434632233504 0.9993813282683883\n"
     ]
    }
   ],
   "source": [
    "x_old = -1\n",
    "eta = 0.01\n",
    "for i in range(400):\n",
    "    x_new = x_old - eta*grad(x_old)\n",
    "    func_value = func(x_new)\n",
    "    grad_value = grad(x_new)\n",
    "    x_old = x_new\n",
    "print(func_value,grad_value,x_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb765452-98c9-415f-8f92-e078fe12b323",
   "metadata": {},
   "source": [
    "**5. Gradient Descent on two parameter linear regression model**\n",
    "\n",
    "Now we can explore the idea of gradient descent in the context of linear regression model of the form $\\frac{1}{2n}\\sum(y_i-(\\theta_0+\\theta_1x_i))^2$\n",
    "\n",
    "**a. Now $grad(\\theta_0)$ for one term will be: (MIT Lectures ignore this term in videos and assume that $\\theta_0=0$)**\n",
    "\n",
    "$\\frac{-1}{n}(y_i-(\\theta_0+\\theta_1x_i)) = \\frac{-1}{n}(E_i)$\n",
    "\n",
    "And $grad(\\theta_0)$ for all terms will be:\n",
    "\n",
    "$\\frac{-1}{n}\\sum(E_i)$\n",
    "\n",
    "**b. $grad(\\theta_1)$ for one term will be:**\n",
    "\n",
    "$\\frac{-1}{n}(y_i-(\\theta_0+\\theta_1x_i))x_i = \\frac{-1}{n}E_ix_i$\n",
    "\n",
    "And $grad(\\theta_1)$ for all terms will be:\n",
    "\n",
    "$\\frac{-1}{n}\\sum(E_ix_i)$\n",
    "\n",
    "You can refer to detailed calculations on a numerical example in the sheet **Grad Descent Linear Regression**\n",
    "\n",
    "**Class Excercise: Use the data in sheet \"One Feature\" and estimate a model using gradient descent**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c80219eb-487e-4263-ab8f-ad1c18ecc679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_excel(\"Numerical Examples.xlsx\",sheet_name = \"One Feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2167f50d-4ea4-4592-8f05-ff26f7bb4ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = data['x'],data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf9cce24-3f95-4952-91de-40bdb8ba8aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8835071612232427 66.70456267858215\n"
     ]
    }
   ],
   "source": [
    "theta_0_old = 10\n",
    "theta_1_old = 40\n",
    "eta = 0.01\n",
    "iters = 200\n",
    "for i in range(iters):\n",
    "    preds = theta_0_old+theta_1_old*x\n",
    "    errors = y-preds\n",
    "    grad_theta_0 = (-1.0/preds.shape[0])*errors.sum()\n",
    "    grad_theta_1 = (-1.0/preds.shape[0])*(errors*x).sum()\n",
    "    theta_0_new = theta_0_old-eta*grad_theta_0\n",
    "    theta_1_new = theta_1_old-eta*grad_theta_1\n",
    "    theta_0_old = theta_0_new\n",
    "    theta_1_old = theta_1_new\n",
    "print(theta_0_new,theta_1_new)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be91647-5e6e-4308-9ee7-10a01c9d4e6c",
   "metadata": {},
   "source": [
    "The MIT lecture 5 also shows gradient descent being done on a single randomly selected row instead of the whole data as shown in the excel sheet and python example above. We can impliment the same by changing the expressions for the $grad(\\theta_0)$ and $grad(\\theta_1)$\n",
    "\n",
    "Since we are using only one data point the expressions will be as follows:\n",
    "\n",
    "$grad(\\theta_0)=$ $\\frac{-1}{n}(y_i-(\\theta_0+\\theta_1x_i)) = \\frac{-1}{n}(E_i)$\n",
    "\n",
    "$grad(\\theta_1)=$ $\\frac{-1}{n}(y_i-(\\theta_0+\\theta_1x_i))x_i = \\frac{-1}{n}E_ix_i$\n",
    "\n",
    "Below is the python code for implimenting the above on the same dataset as used before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aebdeb26-8161-4dc1-b54d-c59e7f7d85a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = data['x'].values,data['y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61c73e4b-f29f-4684-852e-efc3e66828f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(low=0,high=49,size=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afc1771b-9a88-4205-994a-614bd25cf568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38648345760780384 69.56805541925394\n"
     ]
    }
   ],
   "source": [
    "theta_0_old = 10\n",
    "theta_1_old = 40\n",
    "eta = 0.01\n",
    "iters = 20000\n",
    "for i in range(iters):\n",
    "    idx = np.random.randint(low=0,high=49,size=1)[0]\n",
    "    pred = theta_0_old + theta_1_old*x[idx]\n",
    "    error = y[idx] - pred\n",
    "    grad_theta_0 = (-1/50)*error\n",
    "    grad_theta_1 = (-1/50)*error*x[idx]\n",
    "    theta_0_new = theta_0_old-eta*grad_theta_0\n",
    "    theta_1_new = theta_1_old-eta*grad_theta_1\n",
    "    theta_0_old = theta_0_new\n",
    "    theta_1_old = theta_1_new\n",
    "print(theta_0_new,theta_1_new)       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782271b7-a37a-4b32-a8f5-3d018b391663",
   "metadata": {},
   "source": [
    "**5. Closed Form Solution**\n",
    "\n",
    "Another method of estimating the parameters is to find a closed form solution. Before we do that we will make a small change in notation. Instead of talking about parameters separately $\\theta_0$, $\\theta_1$, we will now talk about parameter vector $\\Theta = $ \n",
    "$\\begin{bmatrix}\n",
    "\\theta_0\\\\\n",
    "\\theta_1\\\\\n",
    "\\vdots \\\\\n",
    "\\theta_n\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "The second change we will make is we will represent our features as a feature vector, $X_i$ of the kind $\\begin{bmatrix}\n",
    "            1 \\\\\n",
    "           x_{1} \\\\\n",
    "           x_{2} \\\\\n",
    "           \\vdots \\\\\n",
    "           x_{n}\n",
    "          \\end{bmatrix}$\n",
    "\n",
    "Now our model can be expressed as $\\Theta.X_i$\n",
    "\n",
    "Our cost function will become $\\frac{1}{2n}\\sum(y_i - \\Theta.X_i)^2$\n",
    "\n",
    "This is demonstrated more clearly in the tab named **Vector Notation**\n",
    "\n",
    "\n",
    "**6. Closed form Solution Derivation**\n",
    "\n",
    "Cost function = $C(\\Theta)=\\frac{1}{2n}\\sum(y_i - \\Theta.X_i)^2$,\n",
    "\n",
    "$\\nabla(C(\\Theta)) = \\frac{-1}{n}(\\sum(y_i - \\Theta.X_i)(X_i)$\n",
    "\n",
    "$\\nabla(C(\\Theta)) = \\frac{-1}{n}\\sum(y_iX_i) + \\frac{1}{n}\\sum((\\Theta.X_i)X_i)$\n",
    "\n",
    "$\\nabla(C(\\Theta)) = -b + \\frac{1}{n}\\sum(X_i(\\Theta.X_i))$, now $\\Theta.X_i = X_i^T\\Theta =>$ \n",
    "\n",
    "$\\nabla(C(\\Theta)) = -b + \\frac{1}{n}\\sum((X_iX_i^T)\\Theta) = -b+A\\Theta$\n",
    "\n",
    "To find the optimal value of $\\Theta$, we can try solving $\\nabla(C(\\Theta))=0 =>$\n",
    "\n",
    "$A\\Theta - b = 0=> A\\Theta = b => A^{-1}A\\Theta = A^{-1}b => \\Theta = A^{-1}b$\n",
    "\n",
    "**7. Regularization**\n",
    "\n",
    "Just like we would want to control for overfiting in classification problems, we tend to do the same thing with regression problems as well.\n",
    "\n",
    "MIT curriculum talks about **ridge regression**, in this model the cost function changes as follows:\n",
    "\n",
    "$Cost(\\Theta) = \\frac{\\lambda}{2}||{\\Theta}||^2 + \\frac{1}{2n}\\sum(y_i - \\Theta.X_i)^2$\n",
    "\n",
    "Here $||{\\Theta}||^2 = \\sqrt{(\\theta_0^2+\\theta_1^2+..+\\theta_n^2)}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b74c0830-3d15-48af-ad42-a11c1cb99708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta_0: 3.6579953941065915, Theta_1: [1.38360293]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### This code is to explain the idea of regularization, don't run it on your local machine, this will be explained in the session\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import Ridge\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def get_data(noise = 2):\n",
    "    X,y,coef = make_regression(n_samples = 50, n_features = 1, noise = noise, random_state=42,coef=True)\n",
    "    return X,y,coef\n",
    "def get_model(X,y,lamda = 3):\n",
    "    reg = Ridge(alpha = lamda)\n",
    "    mod = reg.fit(X,y)\n",
    "    theta_1 = mod.coef_\n",
    "    theta_0 = mod.intercept_\n",
    "    return theta_0,theta_1\n",
    "def plot_data(X,y,theta_1,theta_0):\n",
    "    predictions = theta_0+theta_1*X\n",
    "    plot = plt.plot(X,y,\"*b\",X,predictions,\"*r\")\n",
    "    return plot\n",
    "lamda = 600 ## change the value from 5 to 500 and see coefficient estimates\n",
    "noise = 50\n",
    "X,y,coef = get_data(noise=noise)\n",
    "theta_0,theta_1 = get_model(X,y,lamda=lamda)\n",
    "plot = plot_data(X[:,0],y,theta_1,theta_0)\n",
    "plot\n",
    "print(f\"Theta_0: {theta_0}, Theta_1: {theta_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114aef4f-7010-48fc-a918-443cc4cf7b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512ba492-2a96-444f-912f-ad0f15b1e551",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
