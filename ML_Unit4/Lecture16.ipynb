{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e935fa2-6f8b-408f-8c96-6664ca458234",
   "metadata": {},
   "source": [
    "## Lecture 16: Mixture Models: EM Algorithm (MIT Lecture Notes)\n",
    "\n",
    "This is a companion notebook to tackle the **project** for Unit4. The detailed notes are in **Expectation Maximization.pptx**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f982b7f2-2697-4e48-8037-ff80a0259bc7",
   "metadata": {},
   "source": [
    "**EM Algorithm: GMM (Clustering)**\n",
    "\n",
    "This is a boiler-plate template to get you guys started on how Expectation and Maximization steps can be implimented using numpy arrays.\n",
    "\n",
    "Just to emphasize:\n",
    "\n",
    "E Step "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdac1795-ae5f-45b4-91f9-4a141f9ff025",
   "metadata": {},
   "source": [
    "**KMeans Using EM**\n",
    "\n",
    "One can also frame k-means as a special case of EM algorithm.\n",
    "\n",
    "|EM: GMM|EM:Kmeans|\n",
    "|--------|---------|\n",
    "|**E Step:** $p(j,i)$ is computed based on a Gaussian Mixture of cluster centers|**E Step:** We simply find out points which are near the means|\n",
    "|**M Step:** $\\mu$,$\\sigma$ and $p_j$ are updated based on $n_j$ obtained from $p(j,i)$ obtained in previous step| **M Step**: Only $\\mu$ is updated based on the cluster assignments done previous step|\n",
    "\n",
    "Below is the code given as part of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2e74f4e-8253-42cb-a868-56078d663e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import numpy as np\n",
    "from common import GaussianMixture, plot, init\n",
    "\n",
    "\n",
    "def estep(X: np.ndarray, mixture: GaussianMixture) -> np.ndarray:\n",
    "    \"\"\"E-step: Assigns each datapoint to the gaussian component with the\n",
    "    closest mean\n",
    "\n",
    "    Args:\n",
    "        X: (n, d) array holding the data\n",
    "        mixture: the current gaussian mixture\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: (n, K) array holding the soft counts\n",
    "            for all components for all examples\n",
    "\n",
    "        \"\"\"\n",
    "    n, _ = X.shape\n",
    "    K, _ = mixture.mu.shape\n",
    "    post = np.zeros((n, K))\n",
    "\n",
    "    for i in range(n):\n",
    "        tiled_vector = np.tile(X[i, :], (K, 1))\n",
    "        sse = ((tiled_vector - mixture.mu)**2).sum(axis=1)\n",
    "        j = np.argmin(sse)\n",
    "        post[i, j] = 1 ## You can notice here we are simply hard counting which points belong to which clusters based on distance,(line 25 and 26) \n",
    "\n",
    "    return post\n",
    "\n",
    "\n",
    "def mstep(X: np.ndarray, post: np.ndarray) -> Tuple[GaussianMixture, float]:\n",
    "    \"\"\"M-step: Updates the gaussian mixture. Each cluster\n",
    "    yields a component mean and variance.\n",
    "\n",
    "    Args: X: (n, d) array holding the data\n",
    "        post: (n, K) array holding the soft counts\n",
    "            for all components for all examples\n",
    "\n",
    "    Returns:\n",
    "        GaussianMixture: the new gaussian mixture\n",
    "        float: the distortion cost for the current assignment\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    _, K = post.shape\n",
    "\n",
    "    n_hat = post.sum(axis=0)\n",
    "    p = n_hat / n\n",
    "\n",
    "    cost = 0\n",
    "    mu = np.zeros((K, d))\n",
    "    var = np.zeros(K)\n",
    "\n",
    "    for j in range(K):\n",
    "        mu[j, :] = post[:, j] @ X / n_hat[j] ## This step is essentially finding mean of points in each cluster\n",
    "        sse = ((mu[j] - X)**2).sum(axis=1) @ post[:, j]\n",
    "        cost += sse\n",
    "        var[j] = sse / (d * n_hat[j])\n",
    "\n",
    "    return GaussianMixture(mu, var, p), cost\n",
    "\n",
    "\n",
    "def run(X: np.ndarray, mixture: GaussianMixture,\n",
    "        post: np.ndarray) -> Tuple[GaussianMixture, np.ndarray, float]:\n",
    "    \"\"\"Runs the mixture model\n",
    "\n",
    "    Args:\n",
    "        X: (n, d) array holding the data\n",
    "        post: (n, K) array holding the soft counts\n",
    "            for all components for all examples\n",
    "\n",
    "    Returns:\n",
    "        GaussianMixture: the new gaussian mixture\n",
    "        np.ndarray: (n, K) array holding the soft counts\n",
    "            for all components for all examples\n",
    "        float: distortion cost of the current assignment\n",
    "    \"\"\"\n",
    "\n",
    "    prev_cost = None\n",
    "    cost = None\n",
    "    while (prev_cost is None or prev_cost - cost > 1e-4):\n",
    "        prev_cost = cost\n",
    "        post = estep(X, mixture)\n",
    "        mixture, cost = mstep(X, post)\n",
    "\n",
    "    return mixture, post, cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d5ee999-7d04-4c0f-a165-31c7d5a2c350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kmeans(X):\n",
    "    for K in range(1, 5):\n",
    "        min_cost = None\n",
    "        best_seed = None\n",
    "        for seed in range(0, 5):\n",
    "            mixture, post = init(X, K, seed)\n",
    "            mixture, post, cost = run(X, mixture, post)\n",
    "            if min_cost is None or cost < min_cost:\n",
    "                min_cost = cost\n",
    "                best_seed = seed\n",
    "\n",
    "        mixture, post = init(X, K, best_seed)\n",
    "        mixture, post, cost = run(X, mixture, post)\n",
    "        print(f'cost: {cost} k:{K}, seed:{seed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83715e84-7880-4f80-97f7-62be97e4384f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.loadtxt(\"./data/toy_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac693c6f-bb71-4c34-945f-2701be85ce2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 5462.297452340001 k:1, seed:4\n",
      "cost: 1684.9079502962372 k:2, seed:4\n",
      "cost: 1329.5948671544297 k:3, seed:4\n",
      "cost: 1035.499826539466 k:4, seed:4\n"
     ]
    }
   ],
   "source": [
    "run_kmeans(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7d6143-7880-4460-b8b2-105f15253fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
